<p style="color:#0072DD;font-size:2.6rem">
<b>A techie initiator, a problem hacker and a great team player.</b><br>
</p>

<div id="" style="text-align: justify;" markdown="1">
I was a research scientist at <b>KaiKuTeK Inc.</b> in Taiwan. My research goal is to achieve visual intelligence for real-world applications 
and study advanced topics in <b>machine learning</b> and <b>computer vision</b>.
Currently, I am developing gesture recognition with mmWave frequency modulated continuous wave (FMCW) radar, which adopts the range and the velocity estimated from the raw data, 
such as the time-frequency spectrogram, micro-doppler spectrogram, or range-Doppler image (RDI). Meanwhile, this topic also involve few-shot learning, 
long-tailed or rapid gesture recognition task, even sub-actions exploration.
</div>

## <i class="fa fa-chevron-right"></i> Spotlight news
<table class="table table-hover">

<tr>
<td>
<p markdown="1" style='margin: 0'>
<strong>Check out my new article![<img src="./images/com//update.gif">]</strong><br>
</p>

[<a href='javascript:;' onclick='$("#abs_subactionpost").toggle()'>Intro</a>]<br>
<div id="abs_subactionpost" style="text-align: justify;" markdown="1">
This article introduces how I designed Node Attention Module from scratch under certain conditions. 
Our goal is to provide a plug-and-play module to extract sub-actions upon any existing action detection approaches.
<a href='https://lilyo.github.io/2023/03/23/subaction/' target='_blank'><img src="/data/subaction/demo.gif" onerror="this.style.display='none'" style='border: none;' width='1080' height='360' /></a>
</div>

</td>
<td class='col-md-2' style='text-align:right;'>Mar. 23, 2022</td>
</tr>


<tr>
<td>
<p markdown="1" style='margin: 0'>
<strong>Our fast gesture recognition method has been granted a patent in Taiwan. [<img src="./images/com//update.gif">]</strong><br> 
</p>
</td>
<td class='col-md-2' style='text-align:right;'>Dec. 1, 2021</td>
</tr>


<tr>
<td>
<p markdown="1" style='margin: 0'>
<strong>ML Application - TWS Headset with In-Air Gesturing.</strong><br> 
</p>
[<a href='javascript:;' onclick='$("#abs_tws").toggle()'>Intro</a>] <br>
<div id="abs_tws" style="text-align: justify;" markdown="1">
This presentation is a case study where we are demonstrating how we developed gesture-controlled headphones using a radar sensor on Edge AI applications. 
I was assigned as the SDK porject leader to be responsible for this task, and led machine learning team to design gesture set for TWS Headset, build and scale our technology solutions for mmwave radar system.<br>
<div id='outerdiv' style="width:800px; overflow-x:hidden;">
<iframe width="640" height="360" src="https://www.youtube.com/embed/POwsmqwSGcM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
</div>

</td>
<td class='col-md-2' style='text-align:right;'>Nov. 2020 - Mar. 2022</td>
</tr>


<tr>
<td>
<p markdown="1" style='margin: 0'>
<strong>ML Application - ACSIS Smart Charging Cable with In-Air Gesturing</strong><br>
</p>
[<a href='javascript:;' onclick='$("#abs_cable").toggle()'>Intro</a>] <br>
<div id="abs_cable" style="text-align: justify; display: none" markdown="1">
The first commercial product using KaiKuTeK AI solution was launched by the SDK team in 2021. 
It is a symbolic product to push for low power and low-latency deep learning models, computing hardware, and systems for inference on edge devices.<br>
<a href='https://item.m.jd.com/product/10032443440052.html?gx=RnFtl2BdOjzfndQUrIF-XSHfSvY1pY-x&ad_od=share&utm_source=androidapp&utm_medium=appshare&utm_campaign=t_335139774&utm_term=Wxfriends&fbclid=IwAR2Z7i7-_r1LsPFdWGYlHHZS5nzdgimzbd3YGfC__FV7iYRBElljk2QdQeo' target='_blank'><img src="images/application/ACASIS.gif" onerror="this.style.display='none'" style='border: none;' width='640' height='980' /></a>
</div>

</td>
<td class='col-md-2' style='text-align:right;'>Nov. 2020 - Jun. 2021</td>
</tr>


<tr>
<td>
<p markdown="1" style='margin: 0'>
<strong>ML Application - Clone Dual-Bay External Hard Drive Duplicator with In-Air Gesturing</strong><br>
</p>

[<a href='javascript:;' onclick='$("#abs_dup").toggle()'>Intro</a>] <br>
<div id="abs_dup" style="text-align: justify; display: none" markdown="1">
<a href='https://world.taobao.com/item/650812289577.htm?spm=a21wu.10013406-tw.taglist-content.22.68703044FnI8Xv' target='_blank'><img src="images/application/duplicator.jpg" onerror="this.style.display='none'" style='border: none;' width='640' height='980' /></a>
</div>

</td>
<td class='col-md-2' style='text-align:right;'>Nov. 2020 - Jun. 2021</td>
</tr>


<tr>
<td>
<p markdown="1" style='margin: 0'>
<strong>JMicron‧KaiKuTeK - Revolutionary Gesture Recognition 60GHz mmWave Radar Solution</strong><br>
</p>

[<a href='javascript:;' onclick='$("#abs_kkt").toggle()'>Intro</a>] <br>
<div id="abs_kkt" style="text-align: justify; display: none" markdown="1">
<div id='outerdiv' style="width:800px; overflow-x:hidden;">
<iframe width="640" height="360" src="https://www.youtube.com/embed/Ix9PY89ML90" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
</div>
</td>
<td class='col-md-2' style='text-align:right;'>2021</td>
</tr>


</table>

## <i class="fa fa-chevron-right"></i> Experiences
<table class="table table-hover">

<tr>
<td>
<p markdown="1" style='margin: 0'>
<strong>Machine Learning Engineer</strong><br>
KaikuteK Inc. | Taipei<br>
• Research Field<br>
&ensp;&ensp;&ensp;&ensp;- Sub-actions exploration<br>
&ensp;&ensp;&ensp;&ensp;- Rapid gesture recognition<br>
&ensp;&ensp;&ensp;&ensp;- Temporal coherency<br>
• SDK Machine Learning Team Leader<br>
&ensp;&ensp;&ensp;&ensp;- Customers support, lead team members.<br>
&ensp;&ensp;&ensp;&ensp;- Deploy Python API to AWS cloud training platform.<br>
</p>
</td>
<td class='col-md-2' style='text-align:right;'>Feb. 2020 - Mar. 2022</td>
</tr>

<tr>
<td>
<p markdown="1" style='margin: 0'>
<strong>Military Service in Taiwan</strong>
</p>
</td>
<td class='col-md-2' style='text-align:right;'>Oct. 2019 - Feb. 2020</td>
</tr>

<tr>
<td>
<p markdown="1" style='margin: 0'>
<strong>Machine Learning Research Intern</strong><br>
KaikuteK Inc. | Taipei<br>
• Research Field<br>
&ensp;&ensp;&ensp;&ensp;- Long-Tailed Object Recognition<br>
&ensp;&ensp;&ensp;&ensp;- Few-shot Learning<br>
</p>
</td>
<td class='col-md-2' style='text-align:right;'>Jul. 2019 - Oct. 2019</td>
</tr>

</table>


## <i class="fa fa-chevron-right"></i> Education

<table class="table table-hover">
  <tr>
    <td>
        <strong>M.Sc. in Electronic Engineering</strong>
		(4.14/4.30)
        <br>
      National Chung Cheng University | Chiayi
        <p style='margin-top:-1em;margin-bottom:0em' markdown='1'>
        <br> *<a href="https://ieeexplore.ieee.org/document/9413146/figures#figures">DEN: Disentangling and Exchanging Network for Depth Completion</a>*
        <br> Advisor: <a href="http://acm.cs.nctu.edu.tw/Member_Home.aspx?Account=chingchun">Ching-Chun Huang</a>
        </p>
    </td>
    <td class="col-md-2" style='text-align:right;'>2017 - 2019</td>
  </tr>
  <tr>
    <td>
        <strong>B.Sc. in Electronic Engineering</strong>
        <br>
      National Kaohsiung University of Applied Sciences | Kaohsiung
        <p style='margin-top:-1em;margin-bottom:0em' markdown='1'>
        <br> Advisor: <a href="http://ee.nkust.edu.tw/control/jhy-shoung-yaung/">Chih-Hsiung Yang</a>
        </p>
    </td>
    <td class="col-md-2" style='text-align:right;'>2013 - 2017</td>
  </tr>
</table>


## <i class="fa fa-chevron-right"></i> Honors & Awards
<table class="table table-hover">
<tr>
  <td>
    <strong>MS THESIS AWARD HONORABLE MENTION in IPPR</strong><br>
	<p style="color:grey;font-size:1.2rem">
	You-Feng Wu, Vu-Hoang Tran, Ting-Wei Chang, Wei-Chen Chiu, Ching-Chun Huang, "DEN: Disentangling and Exchanging Network for Depth Completion", International Conference on Pattern Recognition(ICPR), Sep., 2020.<br>
	<a href="http://140.125.183.142/res/paperaword/13th/IPPR%E7%AC%AC%E5%8D%81%E4%B8%89%E5%B1%86%E5%8D%9A%E7%A2%A9%E5%A3%AB%E8%AB%96%E6%96%87%E7%8D%8E%E7%8D%B2%E7%8D%8E%E5%85%AC%E5%91%8A-2.pdf">"LINK"</a>
	</p>
  </td>
  <td class='col-md-2' style='text-align:right;'>2020</td>
</tr>
<tr>
  <td>
    <strong>BEST PAPER AWARD in MAPR</strong><br>
	<p style="color:grey;font-size:1.2rem">
		You-Feng Wu, Hoang Tran Vu, Ching-Chun Huang, "Semi-supervised and Multi-task Learning for On-street Parking Space Status Inference", Multimedia Analysis and Pattern Recognition (MAPR), May ., 2019.<br>
	<a href="http://acm.cs.nctu.edu.tw/News.aspx">"LINK"</a>
	</p>
  </td>
  <td class='col-md-2' style='text-align:right;'>2019</td>
</tr>
</table>


## <i class="fa fa-chevron-right"></i> Patents
<table class="table table-hover">
<tr>
  <td>
	<p style="font-size:1.5rem">
		<strong>You-Feng Wu. 2020. IMPULSE-LIKE GESTURE RECOGNITION METHOD, AND IMPULSE-LIKE GESTURE RECOGNITION SYSTEM. Taiwan Patent I748778, filed Dec 1, 2021.</strong><br>
	</p>
	
	<p style="font-size:1.2rem">
	<a href="https://twpat2.tipo.gov.tw/twpatc/twpatkm?.84cc0F1F0100000000010050^000000000002301000000010100A7004285">"LINK"</a>
	</p>
	
  </td>
  <td class='col-md-2' style='text-align:right;'>2021</td>
</tr>
<tr>
  <td>
	<p style="font-size:1.5rem">
    <strong>You-Feng Wu. 2020. IMPULSE-LIKE GESTURE RECOGNITION METHOD, AND IMPULSE-LIKE GESTURE RECOGNITION SYSTEM. U.S. Patent 17/084,986, filed Oct 30, 2020. Patent pending.</strong><br>
	</p>
  </td>
  <td class='col-md-2' style='text-align:right;'>2020</td>
</tr>
</table>


## <i class="fa fa-chevron-right"></i> Publications

<a href="https://scholar.google.com.tw/citations?hl=zh-TW&view_op=list_works&gmla=AJsN-F5cXEm3DcCrNxtM9TcQjGfayIXB1EpvcQM_KRAe9pRiXs8YaUgvpvHdPNWXG_aSHlF8uoMFHSF-7rjGP26BNBDXHKQ0OtM4UYryJS57huM7UUB9FZg&user=0VGDeTUAAAAJ" class="btn btn-primary" style="padding: 0.3em;">
  <i class="ai ai-google-scholar"></i> Google Scholar
</a>

<h2>2020</h2>
<table class="table table-hover">

<tr id="tr-amos2021modelbased" style="background-color: #E5EBF7">
<td class="col-md-3"><a href='https://github.com/Lilyo/DEN' target='_blank'><img src="images/publications/den.png" onerror="this.style.display='none'" style='border: none;' /></a> </td>
<td>
    <em><a href='https://ieeexplore.ieee.org/document/9413146' target='_blank'>DEN: Disentangling and Exchanging Network for Depth Completion</a> </em><br>
    <strong>You-Feng Wu</strong>, Vu-Hoang Tran, Ting-Wei Chang, Wei-Chen Chiu and Ching-Chun Huang<br>
    ICPR 2020<br>
    [1] 
[<a href='javascript:;'
    onclick='$("#abs_den").toggle()'>abs</a>] [<a href='https://github.com/Lilyo/DEN' target='_blank'>code</a>] <br>
    
<div id="abs_den" style="text-align: justify; display: none" markdown="1">
In this paper, we tackle the depth completion problem. Conventional depth sensors usually produce incomplete depth maps due to the property of surface reflection, 
especially for the window areas, metal surfaces, and object boundaries. 
However, we observe that the corresponding RGB images are still dense and preserve all of the useful structural information. 
The observation brings us to the question of whether we can borrow this structural information from RGB images to inpaint the corresponding incomplete depth maps. 
In this paper, we answer that question by proposing a Disentangling and Exchanging Network (DEN) for depth completion. 
The network is designed based on the assumption that after suitable feature disentanglement, RGB images and depth maps share a common domain for representing structural information. 
So we firstly disentangle both RGB and depth images into domain-invariant content parts, which contain structural information, and domain-specific style parts. 
Then, by exchanging the complete structural information extracted from the RGB image with incomplete information extracted from the depth map, we can generate the complete version of the depth map. 
Furthermore, to address the mixed-depth problem, a newly proposed depth representation is applied. 
By modeling depth estimation as a classification problem coupled with coefficient estimation, blurry edges are enhanced in the depth map. 
At last, we have implemented ablation experiments to verify the effectiveness of the proposed DEN model. 
The results also demonstrate the superiority of DEN over some state-of-the-art approaches.

</div>

</td>
</tr>

</table>

<h2>2019</h2>
<table class="table table-hover">

<tr id="tr-amos2021modelbased" style="background-color: #E5EBF7">
<td class="col-md-3"><a href='https://github.com/Lilyo/Parking-Space-Inference' target='_blank'><img src="images/publications/multi_task.gif" onerror="this.style.display='none'" style='border: none;' /></a> </td>
<td>
    <em><a href='https://ieeexplore.ieee.org/document/8743537' target='_blank'>Semi-supervised and Multi-task Learning for On-street Parking Space Status Inference</a> </em><br>
    <strong>You-Feng Wu</strong>, Hoang Tran Vu and Ching-Chun Huang<br>
    MAPR 2019  <br>
    [2] 
[<a href='javascript:;'
    onclick='$("#abs_multi_task").toggle()'>abs</a>] [<a href='https://github.com/Lilyo/Parking-Space-Inference' target='_blank'>code</a>] <br>
    
<div id="abs_multi_task" style="text-align: justify; display: none" markdown="1">
To manage on-street parking spaces, magnetic sensor is often used due to its low cost and flexibility in installation and usage. 
However, its signals are easily affected by environment, vehicle type, installation location and moving neighboring vehicles. 
Besides, accidental installation also leads to non-unified coordinate of magnetic sensors which makes the management system difficult to recognize. 
To overcome these challenges, we proposed a novel semi-supervised and multi-task learning framework for sensor based on-street parking slot inference with three contributions. 
First, a Coordinate Transform Module is integrated into our framework to reduce the diversity of input signals by transforming them adaptively into a unified coordinate. 
Second, to learn the generalized and discriminative features while minimizing the amount of labeled data, we introduce a Multi-task Module to leverage the information from both labeled and unlabeled data. 
Third, we embed a Temporal Module, which observes and memorizes the parking states from time to time, to infer parking space status in a reliable way. 
The experimental results show that, with the proposed three modules, our end-to-end training framework could reduce the error detection and hence improve the system accuracy.
</div>

</td>
</tr>

</table>


## <i class="fa fa-chevron-right"></i> Selected Projects

<h2>2018</h2>
<table class="table table-hover">

<tr id="tr-amos2021modelbased" style="background-color: #E5EBF7">
<td class="col-md-3"><a href='https://lilyo.github.io/' target='_blank'><img src="images/publications/cvae.png" onerror="this.style.display='none'" style='border: none;' /></a> </td>
<td>
    <em><a href='https://lilyo.github.io/' target='_blank'>Scene understanding based navigation system upon deep inference learning</a> </em><br>
    Ting-Wei Lin, Shu-Hsien Huang, <strong>You-Feng Wu</strong> and Ching-Chun Huang<br>
     Feb. 2018 - Jun. 2018 <br>
    [3] 
[<a href='javascript:;'
    onclick='$("#abs_cvae").toggle()'>abs</a>]<br>
    
<div id="abs_cvae" style="text-align: justify; display: none" markdown="1">
Traditionally, Sampling based motion planning (SBMP) has emerged as a successful algorithmic paradigm for solving high
dimensional, complex, and dynamically constrained motion planning problems.
However, the performance of SBMP is tied to the placement of samples in these promising regions, a result uniform sampling is only
able to achieve through sheer exhaustion.
We proposed a methodology for non uniform sampling which can improve the convergence speed of traditional particle based scattering algorithm
</div>
</td>
</tr>

<tr id="tr-amos2021modelbased" style="background-color: #E5EBF7">
<td class="col-md-3"><a href='https://lilyo.github.io/' target='_blank'><img src="images/publications/medical_segmentation.png" onerror="this.style.display='none'" style='border: none;' /></a> </td>
<td>
    <em><a href='https://lilyo.github.io/' target='_blank'>Inception U-net based medical image segmentation.</a> </em><br>
    <strong>You-Feng Wu</strong> and Ching-Chun Huang<br>
    Jul. 2017 - Feb. 2018  <br>
    [4] 
[<a href='javascript:;'
    onclick='$("#abs_seg").toggle()'>abs</a>]<br>
    
<div id="abs_seg" style="text-align: justify; display: none" markdown="1">
Most of the image segmentation task is completed by U net. For better performance, we combine U-net and inception module for retina segmentation
</div>

</td>
</tr>

</table>

<h2>2017</h2>
<table class="table table-hover">

<tr id="tr-amos2021modelbased" style="background-color: #E5EBF7">
<td class="col-md-3"><a href='https://github.com/Lilyo/EBMA' target='_blank'><img src="images/publications/ebma.gif" onerror="this.style.display='none'" style='border: none;' /></a> </td>
<td>
    <em><a href='https://github.com/Lilyo/EBMA' target='_blank'>The Study of Integer-/Half-Pixel Exhaustive Block Matching Algorithm for Motion Estimation.</a> </em><br>
    <strong>You-Feng Wu</strong> and Jui-Chiu Chiang<br>
    Nov. 2017<br>
    [5] 
[<a href='javascript:;'
    onclick='$("#abs_ebma").toggle()'>abs</a>] [<a href='https://github.com/Lilyo/EBMA' target='_blank'>code</a>] <br>
    
<div id="abs_ebma" style="text-align: justify; display: none" markdown="1">
Assume that the adjacent frames are similar and change are due to object or camera motion, we can predict a new frame from a previous frame and only code the prediction error.
</div>

</td>
</tr>

<tr id="tr-amos2021modelbased" style="background-color: #E5EBF7">
<td class="col-md-3"><a href='http://acm.cs.nctu.edu.tw/Demo.aspx?c=4' target='_blank'><img src="images/publications/slam.gif" onerror="this.style.display='none'" style='border: none;' /></a> </td>
<td>
    <em><a href='http://acm.cs.nctu.edu.tw/Demo.aspx?c=4' target='_blank'>ORB-SLAM2 runs in real-time on an NVIDIA Jetson TX2.</a> </em><br>
    <strong>You-Feng Wu</strong> and Ching-Chun Huang<br>
    Feb. 2017 - Nov. 2017<br>
	MOST-107-2622-E194-007-CC3<br>
    [6] 
[<a href='javascript:;'
    onclick='$("#abs_slam").toggle()'>abs</a>] [<a href='https://www.youtube.com/watch?v=LuUTbhfWlVQ' target='_blank'>video</a>]<br>
    
<div id="abs_slam" style="text-align: justify; display: none" markdown="1">
Nowadays, there are many service robots in the market; however, only few of them become a popular product. 
Among them, the vacuum cleaning robot might be the most successful one and treated as the key entry point toward the future market of service robots. 
In order to enable the intelligent function in a cleaning robot, the ability for a robot to Simultaneous Localization and Mapping (SLAM) is the fundamental and critical step. 
Hence, in this project, we aim to study and implement the SLAM algorithm in a cleaning robot.
</div>

</td>
</tr>

</table>


## <i class="fa fa-chevron-right"></i> Invited Talks
<table class="table table-hover">
<tr>
  <td>
        <strong>Machine Learning Course - Fine-tuning : Essential Training</strong><br>
		Artificial intelligence and machine learning are changing the world. In this lecture, we are going to introduce: (1)Everything a marketer needs to know about machine learning, (2) How to efficiently fine-tune model.
  </td>
  <td class='col-md-1' style='text-align:right;'>2021</td>
</tr>
</table>


## <i class="fa fa-chevron-right"></i> Skills
<table class="table table-hover">
<tr>
  <td class='col-md-2'>Programming Languages</td>
  <td>
Python, MATLAB, C, C++, JAVA
  </td>
</tr>
<tr>
  <td class='col-md-2'>Deep Learning Frameworks</td>
  <td>
PyTorch, TensorFlow, Caffe
  </td>
</tr>
<tr>
  <td class='col-md-2'>GUI Tools</td>
  <td>
PyQt5, wxPython
  </td>
</tr>
<tr>
  <td class='col-md-2'>Misc.</td>
  <td>
Linux, vim, git, tmux, LATEX
  </td>
</tr>
</table>
